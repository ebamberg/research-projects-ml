{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkAnctd0fyodqpKa49YNnX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ebamberg/research-projects-ml/blob/main/LLM/training/examples_fine_tune_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pub.towardsai.net/fine-tuning-llms-from-zero-to-hero-with-python-ollama-52258966bb6d"
      ],
      "metadata": {
        "id": "-5XCZEadNcUq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdaAkiZzNMNu"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install unsloth\n",
        "# !pip install \"unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps trl peft accelerate bitsandbytes transformers datasets\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the basemodel in unsloth"
      ],
      "metadata": {
        "id": "pyelZNmgN31B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model_name = \"unsloth/phi-3-mini-4k-instruct-bnb-4bit\"\n",
        "max_seq_length = 2048  # Adjust based on your data length\n",
        "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True  # Use 4bit quantization to reduce memory usage\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")"
      ],
      "metadata": {
        "id": "QRWsICdXNzVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RWjo9HjJNboR"
      }
    }
  ]
}