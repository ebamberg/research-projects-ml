{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/ebamberg/research-projects-ml/blob/main/LLM_training_examples_fine_tune_training.ipynb",
      "authorship_tag": "ABX9TyPrOyJjQgNUgocDbCZhN2Wz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ebamberg/research-projects-ml/blob/main/LLM/training/examples_fine_tune_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downgrade protobuf to a compatible version - otherwise save_pretrained_gguf fails on google colab\n",
        "!pip install \"protobuf>=3.19.0,<4.0.0\" --quiet\n",
        "# also for google colab\n",
        "import os\n",
        "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
      ],
      "metadata": {
        "id": "k_Wtva3ORUbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdaAkiZzNMNu"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install unsloth --quiet\n",
        "# !pip install \"unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps trl peft accelerate bitsandbytes transformers datasets --quiet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['WANDB_API_KEY']=userdata.get('WANDB_API_KEY')\n"
      ],
      "metadata": {
        "id": "24uIs8kMsRmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the basemodel in unsloth"
      ],
      "metadata": {
        "id": "pyelZNmgN31B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model_name = \"unsloth/phi-3-mini-4k-instruct-bnb-4bit\"\n",
        "max_seq_length = 2048  # Adjust based on your data length\n",
        "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True  # Use 4bit quantization to reduce memory usage\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")"
      ],
      "metadata": {
        "id": "QRWsICdXNzVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add LoRA Adapter to the layers for efficient training"
      ],
      "metadata": {
        "id": "g2YYhDjJRp0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # Rank - higher = more parameters\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407\n",
        ")"
      ],
      "metadata": {
        "id": "sUw7qpsqRmnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training dataset that we read in looks like:\n",
        "\n",
        "[\n",
        "{\"input\",\"our input\", \"output\", \"expected output\"},\n",
        "{\"input\",\"our input\", \"output\", \"expected output\"},\n",
        "{\"input\",\"our input\", \"output\", \"expected output\"},\n",
        "]"
      ],
      "metadata": {
        "id": "to26wIm0Q-xK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-cache https://raw.githubusercontent.com/ebamberg/research-projects-ml/refs/heads/main/LLM/training/logfile_data_synthetic_train.json -O training_data.json\n",
        "!wget --no-cache https://raw.githubusercontent.com/ebamberg/research-projects-ml/refs/heads/main/LLM/training/logfile_data_synthetic_eval.json -O eval_data.json"
      ],
      "metadata": {
        "id": "-HGviMljobqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "with open(\"training_data.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "# Format for training\n",
        "def format_chat_template(item):\n",
        "    return tokenizer.apply_chat_template(\n",
        "        [\n",
        "            {\"role\": \"user\", \"content\": item['input']},\n",
        "            {\"role\": \"assistant\", \"content\": item['output']}\n",
        "        ],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "# Create the training dataset\n",
        "formatted_data = [{\"text\": format_chat_template(item)} for item in data]\n",
        "dataset = Dataset.from_list(formatted_data)\n",
        "# Check what it looks like\n",
        "print(\"Sample training example:\")\n",
        "print(formatted_data[0][\"text\"])"
      ],
      "metadata": {
        "id": "lfHSByrIOhDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RWjo9HjJNboR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are ready to go, so start the training !"
      ],
      "metadata": {
        "id": "QFbEhV6OR575"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "import torch\n",
        "\n",
        "training_steps=60\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=1, # Reduced batch size\n",
        "        gradient_accumulation_steps=8, # Increased accumulation steps to maintain similar effective batch size\n",
        "        warmup_steps=5,\n",
        "        max_steps=training_steps,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",  # Use \"adamw_torch\" if you get optimizer errors\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        save_steps=30,\n",
        "    ),\n",
        ")\n",
        "# Start training! ðŸš€\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "yn1p_wveR47j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate our model"
      ],
      "metadata": {
        "id": "WriPkkwSpix2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "expected output: Suspicious, Suspious, Nornal"
      ],
      "metadata": {
        "id": "J3mvtx18uL4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_prompts = [\"classify the following log event sequence as normal or suspicious\\n91.189.89.199,sess_752130,root,2025-07-27 14:22:26,Authentication failed for user root,ERROR\\n185.220.101.42,sess_752130,root,2025-07-27 14:24:26,Authentication failed for user root,ERROR\\n185.220.101.42,sess_752130,root,2025-07-27 14:26:26,Authentication failed for user root,ERROR\\n185.220.101.42,sess_752130,root,2025-07-27 14:28:26,Authentication failed for user root,ERROR\\n185.220.101.42,sess_752130,root,2025-07-27 14:30:26,Authentication failed for user root,ERROR\\n91.189.89.199,sess_752130,root,2025-07-27 14:32:26,Authentication failed for user root,ERROR\\n91.189.89.199,sess_752130,root,2025-07-27 14:34:26,Authentication failed for user root,ERROR\\n185.220.101.42,sess_752130,root,2025-07-27 14:36:26,Authentication failed for user root,ERROR\\n91.189.89.199,sess_752130,root,2025-07-27 14:42:26,Brute force attack identified,ERROR\",\n",
        "                \"classify the following log event sequence as normal or suspicious\\n77.88.55.80,sess_256163,emma.taylor,2025-07-30 14:22:26,User authentication successful,INFO\\n77.88.55.80,sess_256163,emma.taylor,2025-07-30 14:24:26,Database query executed,INFO\\n77.88.55.80,sess_256163,emma.taylor,2025-07-30 14:26:26,Abnormal data access volume,ERROR\\n77.88.55.80,sess_256163,emma.taylor,2025-07-30 14:28:26,Data exfiltration pattern,ERROR\\n77.88.55.80,sess_256163,emma.taylor,2025-07-30 14:30:26,Suspicious file access pattern,ERROR\\n77.88.55.80,sess_256163,emma.taylor,2025-07-30 14:32:26,Network intrusion detected,ERROR\",\n",
        "                \"classify the following log event sequence as normal or suspicious\\n192.168.1.10,sess_750740,kevin.lee,2025-07-24 14:22:26,User authentication successful,INFO\\n192.168.1.10,sess_750740,kevin.lee,2025-07-24 14:24:26,Form submission processed,INFO\\n192.168.1.10,sess_750740,kevin.lee,2025-07-24 14:24:26,Resource access granted,INFO\\n192.168.1.10,sess_750740,kevin.lee,2025-07-24 14:25:26,Session timeout warning,DEBUG\\n192.168.1.10,sess_750740,kevin.lee,2025-07-24 14:37:26,Session started,DEBUG\\n192.168.1.10,sess_750740,kevin.lee,2025-07-24 14:40:26,Session timeout warning,DEBUG\\n192.168.1.10,sess_750740,kevin.lee,2025-07-24 14:42:26,User logout successful,DEBUG\\n192.168.1.10,sess_750740,kevin.lee,2025-07-24 15:02:26,User logout successful,INFO\",\n",
        "        ]\n",
        "print(eval_prompts)"
      ],
      "metadata": {
        "id": "_6zn72sTqlg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch to inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "for test in eval_prompts:\n",
        "  print(test)\n",
        "\n",
        "  # Format input using chat template\n",
        "  messages = [{\"role\": \"user\", \"content\": test}]\n",
        "  inputs = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      tokenize=True,\n",
        "      add_generation_prompt=True,\n",
        "      return_tensors=\"pt\"\n",
        "  ).to(\"cuda\")\n",
        "  # Generate response\n",
        "  with torch.no_grad():\n",
        "      outputs = model.generate(\n",
        "          input_ids=inputs,\n",
        "          max_new_tokens=256,\n",
        "          use_cache=True,\n",
        "          temperature=0.1,\n",
        "          do_sample=True,\n",
        "          pad_token_id=tokenizer.eos_token_id\n",
        "      )\n",
        "  # Extract just the new generated text (not the full conversation)\n",
        "  response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
        "  print(\"Model output:\")\n",
        "  print(response.strip())\n"
      ],
      "metadata": {
        "id": "Qlcujay7phos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SAVE our fine-tuned model in the GGUF format which is compatible to Ollama."
      ],
      "metadata": {
        "id": "vstCcbG1UMSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save_pretrained_gguf is a unsloth function. this is not available on standard hugging face models\n",
        "model.save_pretrained_gguf(\n",
        "    \"fine_tuned_model\",\n",
        "    tokenizer,\n",
        "    quantization_method=\"q4_k_m\"  # Good balance of size/quality\n",
        ")"
      ],
      "metadata": {
        "id": "ImTXkUv_ULqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ./fine_tuned_model/Modelfile.py\n",
        "FROM ./unsloth.Q4_K_M.gguf\n",
        "PARAMETER temperature 0.1\n",
        "PARAMETER top_p 0.9\n",
        "PARAMETER stop [\"<|endoftext|>\"]\n",
        "TEMPLATE \"{{ .Prompt }}\"\n",
        "SYSTEM \"You are a specialized devop trained to analyze log file.\""
      ],
      "metadata": {
        "id": "SDKpla0sU_oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/fine_tuned_model_log_file_analyser.zip /content/fine_tuned_model/\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/fine_tuned_model_log_file_analyser.zip\")"
      ],
      "metadata": {
        "id": "yDjohjwRcK6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "you can use ollama to create a model from the modelfile:\n",
        "\n",
        "ollama create \\<modelname\\> -f Modelfile"
      ],
      "metadata": {
        "id": "oUfANovcVuRd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1928845"
      },
      "source": [
        "!pip install ollama --quiet\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/fine_tuned_model/\n",
        "get_ipython().system_raw(\"ollama create log_file_analyser -f Modelfile\")"
      ],
      "metadata": {
        "id": "OYVznuQvaJsl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}