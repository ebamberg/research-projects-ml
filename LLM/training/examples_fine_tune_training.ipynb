{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRUKCyprvbw5DAC9ygaNNq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ebamberg/research-projects-ml/blob/main/LLM/training/examples_fine_tune_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pub.towardsai.net/fine-tuning-llms-from-zero-to-hero-with-python-ollama-52258966bb6d"
      ],
      "metadata": {
        "id": "-5XCZEadNcUq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdaAkiZzNMNu"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install unsloth\n",
        "# !pip install \"unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps trl peft accelerate bitsandbytes transformers datasets\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the basemodel in unsloth"
      ],
      "metadata": {
        "id": "pyelZNmgN31B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model_name = \"unsloth/phi-3-mini-4k-instruct-bnb-4bit\"\n",
        "max_seq_length = 2048  # Adjust based on your data length\n",
        "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True  # Use 4bit quantization to reduce memory usage\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")"
      ],
      "metadata": {
        "id": "QRWsICdXNzVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add LoRA Adapter to the layers for efficient training"
      ],
      "metadata": {
        "id": "g2YYhDjJRp0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # Rank - higher = more parameters\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407\n",
        ")"
      ],
      "metadata": {
        "id": "sUw7qpsqRmnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training dataset that we read in looks like:\n",
        "\n",
        "[\n",
        "{\"input\",\"our input\", \"output\", \"expected output\"},\n",
        "{\"input\",\"our input\", \"output\", \"expected output\"},\n",
        "{\"input\",\"our input\", \"output\", \"expected output\"},\n",
        "]"
      ],
      "metadata": {
        "id": "to26wIm0Q-xK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "with open(\"training_dataset.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "# Format for training - using chat template\n",
        "def format_chat_template(item):\n",
        "    return tokenizer.apply_chat_template(\n",
        "        [\n",
        "            {\"role\": \"user\", \"content\": item['input']},\n",
        "            {\"role\": \"assistant\", \"content\": json.dumps(item['output'])}\n",
        "        ],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "# Create the training dataset\n",
        "formatted_data = [{\"text\": format_chat_template(item)} for item in data]\n",
        "dataset = Dataset.from_list(formatted_data)\n",
        "# Check what it looks like\n",
        "print(\"Sample training example:\")\n",
        "print(formatted_data[0][\"text\"])"
      ],
      "metadata": {
        "id": "lfHSByrIOhDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RWjo9HjJNboR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are ready to go, so start the training !"
      ],
      "metadata": {
        "id": "QFbEhV6OR575"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_steps=60\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=training_steps,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",  # Use \"adamw_torch\" if you get optimizer errors\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        save_steps=30,\n",
        "    ),\n",
        ")\n",
        "# Start training! ðŸš€\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "yn1p_wveR47j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}