{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDAufbcSoanLUEi5PqCLOh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ebamberg/research-projects-ml/blob/main/utilities/examples_save_Load_with_safetensors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Safetensors can be used to save and load tensors in a safe and performant way.\n",
        "What do we mean by safe ?\n",
        "With safe we mean Safetensor doesn't allow the execution of arbitrary code while loading of data.\n",
        "But Safetensors has more benefits just like that. The library supports lazy loading, so we can inspects which tensors are in a file without loading the whole file and and just load required tensors.\n",
        "Zero-copy: safetensor doesn't requite more memory when reading then the original file.\n",
        "\n",
        "First let us create two tensors as a dictionary"
      ],
      "metadata": {
        "id": "ywkX9kGnYtBQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "699jkUPkYC1l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from safetensors.torch import save_file, safe_open\n",
        "\n",
        "tensors = {\n",
        "    \"embedding\": torch.zeros((2, 2)),\n",
        "    \"attention\": torch.zeros((2, 3))\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use the safetensor function save_file to save the dictionary to a file."
      ],
      "metadata": {
        "id": "z79_GHOjZFL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_file(tensors, \"model.safetensors\")"
      ],
      "metadata": {
        "id": "GDlfPu7jYXRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading in the file is simple as well\n",
        "\n"
      ],
      "metadata": {
        "id": "DGa-QLq_Z1pT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensors = {}\n",
        "with safe_open(\"model.safetensors\", framework=\"pt\") as f:    # , device=0 as parameter when we\n",
        "    for k in f.keys():\n",
        "        tensors[k] = f.get_tensor(k) # loads the full tensor given a key\n",
        "print(tensors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pf_ObehaIvT",
        "outputId": "2e485496-312c-421b-e3a4-67984b83f951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'attention': tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]]), 'embedding': tensor([[0., 0.],\n",
            "        [0., 0.]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "if we want to load the tensor directly into GPU we can add a \"device\" parameter\n",
        "\n",
        "with safe_open(\"model.safetensors\", framework=\"pt\", device=0) as f:"
      ],
      "metadata": {
        "id": "gn2Qx8DJaZIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SafeTensors also supports lazy loading and loading of parts of the files. This is helpful for large file for example to avoid loading meta data.\n",
        "\n",
        "We can also just load a slice of a tensor"
      ],
      "metadata": {
        "id": "wDjD_oJGbFGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensors = {}\n",
        "with safe_open(\"model.safetensors\", framework=\"pt\") as f:\n",
        "    tensor_slice = f.get_slice(\"embedding\")\n",
        "    vocab_size, hidden_dim = tensor_slice.get_shape()\n",
        "    tensor = tensor_slice[:, :hidden_dim-1]\n",
        "print (tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5lAvJigapzJ",
        "outputId": "68f75815-b760-4576-f8e1-61124e225e92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.],\n",
            "        [0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "loading of model state is also directly supported by the library. Instead of calling model.state_dict() and save the dictionary we can simply pass in the model to the save_model() model. for loading the state dictionary we can use the function load_model()"
      ],
      "metadata": {
        "id": "ifo_tRSzb0ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_model, save_model\n",
        "\n",
        "save_model(model, \"model.safetensors\")\n",
        "# Instead of save_file(model.state_dict(), \"model.safetensors\")\n",
        "\n",
        "load_model(model, \"model.safetensors\")\n",
        "# Instead of model.load_state_dict(load_file(\"model.safetensors\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "meSQguO4cChQ",
        "outputId": "c52fcf43-d4e0-4572-a19a-8fd3d55137fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-463888ec0646>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msafetensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model.safetensors\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Instead of save_file(model.state_dict(), \"model.safetensors\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    }
  ]
}