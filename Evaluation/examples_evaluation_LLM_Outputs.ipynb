{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7pUunnQuNbz2xWT+66Zku",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ebamberg/research-projects-ml/blob/main/Evaluation/examples_evaluation_LLM_Outputs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# # Evaluating LLM Output\n",
        "\n",
        "F1-Score (Precision/Recall)\n",
        "ROUGE\n",
        "BLEU\n",
        "\n",
        "Use BLEU when evaluating machine translation tasks, where precision and fluency are critical.\n",
        "Use ROUGE for summarization tasks where capturing key ideas and recall is more important than exact wording.\n",
        "\n",
        "AUC-ROC- distinguised between classes\n",
        "\n",
        "|LLM task        | evaluation metric | frameworks|\n",
        "|----------------|-------------------|-----------|\n",
        "| Classification | F1-Score, AUC-ROC |           |\n",
        "| Regression     | Mean Squared Error (MSE),Root Mean Squared Error (RMSE),Mean Absolute Error (MAE),R-squared (RÂ²)                  |           |\n",
        "| Summarization  | ROUGE             | rouge-score, evaluate      |\n",
        "| Translation    | BLEU              | nltk, evaluate, sacrebleu |\n"
      ],
      "metadata": {
        "id": "FqKtTGCXw-CY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ewl8CHOQw9SL"
      },
      "outputs": [],
      "source": []
    }
  ]
}