{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6zGiyyKp4uTEeKBE4s8W8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ebamberg/research-projects-ml/blob/main/agents/langgraph/tool_calls.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpHwp4UW6xjc"
      },
      "outputs": [],
      "source": [
        "# Step 1: Define tools and model\n",
        "\n",
        "from langchain.tools import tool\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "\n",
        "model = init_chat_model(\n",
        "    \"claude-sonnet-4-5-20250929\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "\n",
        "# Define tools\n",
        "@tool\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply `a` and `b`.\n",
        "\n",
        "    Args:\n",
        "        a: First int\n",
        "        b: Second int\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "@tool\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Adds `a` and `b`.\n",
        "\n",
        "    Args:\n",
        "        a: First int\n",
        "        b: Second int\n",
        "    \"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "@tool\n",
        "def divide(a: int, b: int) -> float:\n",
        "    \"\"\"Divide `a` and `b`.\n",
        "\n",
        "    Args:\n",
        "        a: First int\n",
        "        b: Second int\n",
        "    \"\"\"\n",
        "    return a / b\n",
        "\n",
        "\n",
        "# Augment the LLM with tools\n",
        "tools = [add, multiply, divide]\n",
        "tools_by_name = {tool.name: tool for tool in tools}\n",
        "model_with_tools = model.bind_tools(tools)\n",
        "\n",
        "# Step 2: Define state\n",
        "\n",
        "from langchain.messages import AnyMessage\n",
        "from typing_extensions import TypedDict, Annotated\n",
        "import operator\n",
        "\n",
        "\n",
        "class MessagesState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], operator.add]\n",
        "    llm_calls: int\n",
        "\n",
        "# Step 3: Define model node\n",
        "from langchain.messages import SystemMessage\n",
        "\n",
        "\n",
        "def llm_call(state: dict):\n",
        "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
        "\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            model_with_tools.invoke(\n",
        "                [\n",
        "                    SystemMessage(\n",
        "                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
        "                    )\n",
        "                ]\n",
        "                + state[\"messages\"]\n",
        "            )\n",
        "        ],\n",
        "        \"llm_calls\": state.get('llm_calls', 0) + 1\n",
        "    }\n",
        "\n",
        "\n",
        "# Step 4: Define tool node\n",
        "\n",
        "from langchain.messages import ToolMessage\n",
        "\n",
        "\n",
        "def tool_node(state: dict):\n",
        "    \"\"\"Performs the tool call\"\"\"\n",
        "\n",
        "    result = []\n",
        "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
        "        tool = tools_by_name[tool_call[\"name\"]]\n",
        "        observation = tool.invoke(tool_call[\"args\"])\n",
        "        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n",
        "    return {\"messages\": result}\n",
        "\n",
        "# Step 5: Define logic to determine whether to end\n",
        "\n",
        "from typing import Literal\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "\n",
        "# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\n",
        "def should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\n",
        "    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n",
        "\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "\n",
        "    # If the LLM makes a tool call, then perform an action\n",
        "    if last_message.tool_calls:\n",
        "        return \"tool_node\"\n",
        "\n",
        "    # Otherwise, we stop (reply to the user)\n",
        "    return END\n",
        "\n",
        "# Step 6: Build agent\n",
        "\n",
        "# Build workflow\n",
        "agent_builder = StateGraph(MessagesState)\n",
        "\n",
        "# Add nodes\n",
        "agent_builder.add_node(\"llm_call\", llm_call)\n",
        "agent_builder.add_node(\"tool_node\", tool_node)\n",
        "\n",
        "# Add edges to connect nodes\n",
        "agent_builder.add_edge(START, \"llm_call\")\n",
        "agent_builder.add_conditional_edges(\n",
        "    \"llm_call\",\n",
        "    should_continue,\n",
        "    [\"tool_node\", END]\n",
        ")\n",
        "agent_builder.add_edge(\"tool_node\", \"llm_call\")\n",
        "\n",
        "# Compile the agent\n",
        "agent = agent_builder.compile()\n",
        "\n",
        "\n",
        "from IPython.display import Image, display\n",
        "# Show the agent\n",
        "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))\n",
        "\n",
        "# Invoke\n",
        "from langchain.messages import HumanMessage\n",
        "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
        "messages = agent.invoke({\"messages\": messages})\n",
        "for m in messages[\"messages\"]:\n",
        "    m.pretty_print()"
      ]
    }
  ]
}