{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ebamberg/research-projects-ml/blob/main/Image_Generation/examples_diffusers_train_a_diffuser_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-gF--zU6Txs"
      },
      "source": [
        "# Train a Diffuser Model\n",
        "## Training Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers[training] --quiet\n",
        "# !pip install --upgrade  diffusers[training] accelerate transformers datasets safetensors --quiet"
      ],
      "metadata": {
        "id": "broNgNd0NPtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjSmzoa16Shi"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "image_size=128\n",
        "batch_size=16\n",
        "\n",
        "dataset_name = \"huggan/smithsonian_butterflies_subset\"\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "fig, axs = plt.subplots(2, 4, figsize=(16, 4))\n",
        "for i, image in enumerate(dataset[:10][\"image\"]):\n",
        "    axs[i//4-1][i-(i//4)*4].imshow(image)\n",
        "    axs[i//4-1][i-(i//4)*4].set_axis_off()\n",
        "fig.show()\n",
        "\n",
        "\n",
        "# resize all images to the same size\n",
        "# the transform function receives a batch of rows from dataset\n",
        "# in torchvision we can chain transform operations by creating a compose.\n",
        "preprocess = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.RandomHorizontalFlip(),             # gives us virtually more examples for training\n",
        "        transforms.ToTensor(),                         # we need a tensor image (ch,w,h) instead of a PIL image (w,h,c)\n",
        "        transforms.Normalize([0.5], [0.5]),\n",
        "    ]\n",
        ")\n",
        "def transform(batch):\n",
        "    # from the batch use column \"image\" and covert to RGB format\n",
        "    images = [preprocess(image.convert(\"RGB\")) for image in batch[\"image\"]]\n",
        "    # return a new dict that only  contains the batch of images\n",
        "    return {\"images\": images}\n",
        "\n",
        "dataset.set_transform(transform)\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print (dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bv5caTZVKjdA"
      },
      "outputs": [],
      "source": [
        "from diffusers import UNet2DModel\n",
        "\n",
        "model = UNet2DModel(\n",
        "    sample_size=image_size,  # the target image resolution\n",
        "    in_channels=3,  # the number of input channels, 3 for RGB images\n",
        "    out_channels=3,  # the number of output channels\n",
        "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
        "    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block\n",
        "    down_block_types=(\n",
        "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
        "        \"DownBlock2D\",\n",
        "    ),\n",
        "    up_block_types=(\n",
        "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
        "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_t7WJrrLwE5"
      },
      "source": [
        "## add noise to an image using a scheduler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL.Image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def display_sample(sample):\n",
        "    image_processed = sample.cpu().permute(0, 2, 3, 1) # resort dimension, first the pixels and the channels last\n",
        "    image_processed = (image_processed + 1.0) * 127.5  # make bytes between -127 and 127\n",
        "    image_processed = image_processed.numpy().astype(np.uint8) #convert to numpy cause this is what PIL.IMage wants\n",
        "\n",
        "    image_pil = PIL.Image.fromarray(image_processed[0]) # create an image. first dimension is the batch so we ignore this\n",
        "    display(image_pil)"
      ],
      "metadata": {
        "id": "ya3ZY1AOOK3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0h2jHJaLGNx"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from diffusers import DDPMScheduler\n",
        "\n",
        "sample_image = dataset[0][\"images\"].unsqueeze(0)\n",
        "print(sample_image.shape)\n",
        "\n",
        "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
        "noise = torch.randn(sample_image.shape)\n",
        "timesteps = torch.LongTensor([50])\n",
        "noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)\n",
        "\n",
        "display_sample(noisy_image)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpT8RY54MI0k"
      },
      "source": [
        "The training objective of the model is to predict the noise added to the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tU4sFW0MKCd"
      },
      "outputs": [],
      "source": [
        "from torch.nn.functional import mse_loss\n",
        "\n",
        "noise_residual = model(noisy_image, timesteps).sample\n",
        "loss = mse_loss(noise_residual, noise)\n",
        "\n",
        "print (loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOV7_f4KV14g"
      },
      "source": [
        "ok no create an optimizer and a leraning rate scheduler and build a training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "7b098856827842df86727794ca69d1ec"
          ]
        },
        "id": "pqqWx00_Vtac",
        "outputId": "653b4ea4-bdb3-4123-bb62-3c34211e29bd"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.2/333.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n",
            "Epochs 50 : : 5000it [06:25, 12.57it/s, epoch=4, loss=0.0676, lr=6.3e-5, step=62] "
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b098856827842df86727794ca69d1ec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpochs 50 : : 5000it [06:36, 12.57it/s, epoch=4, loss=0.0676, lr=6.3e-5, step=62]"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "from torch.nn.functional import mse_loss\n",
        "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
        "from diffusers import DDPMScheduler\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from diffusers import DDPMPipeline\n",
        "from diffusers.utils import make_image_grid\n",
        "import os\n",
        "from safetensors.torch import save_model\n",
        "\n",
        "\n",
        "image_size=128\n",
        "batch_size=16\n",
        "eval_batch_size = 16  # how many images to sample during evaluation\n",
        "num_epochs = 50\n",
        "gradient_accumulation_steps = 1\n",
        "learning_rate = 1e-4\n",
        "lr_warmup_steps = 500\n",
        "\n",
        "dataset_name = \"huggan/smithsonian_butterflies_subset\"\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "fig, axs = plt.subplots(2, 4, figsize=(16, 4))\n",
        "for i, image in enumerate(dataset[:10][\"image\"]):\n",
        "    axs[i//4-1][i-(i//4)*4].imshow(image)\n",
        "    axs[i//4-1][i-(i//4)*4].set_axis_off()\n",
        "fig.show()\n",
        "\n",
        "\n",
        "# resize all images to the same size\n",
        "# the transform function receives a batch of rows from dataset\n",
        "# in torchvision we can chain transform operations by creating a compose.\n",
        "preprocess = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.RandomHorizontalFlip(),             # gives us virtually more examples for training\n",
        "        transforms.ToTensor(),                         # we need a tensor image (ch,w,h) instead of a PIL image (w,h,c)\n",
        "        transforms.Normalize([0.5], [0.5]),\n",
        "    ]\n",
        ")\n",
        "def transform(batch):\n",
        "    # from the batch use column \"image\" and covert to RGB format\n",
        "    images = [preprocess(image.convert(\"RGB\")) for image in batch[\"image\"]]\n",
        "    # return a new dict that only  contains the batch of images\n",
        "    return {\"images\": images}\n",
        "\n",
        "dataset.set_transform(transform)\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
        "\n",
        "from diffusers import UNet2DModel\n",
        "\n",
        "model = UNet2DModel(\n",
        "    sample_size=image_size,  # the target image resolution\n",
        "    in_channels=3,  # the number of input channels, 3 for RGB images\n",
        "    out_channels=3,  # the number of output channels\n",
        "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
        "    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block\n",
        "    down_block_types=(\n",
        "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
        "        \"DownBlock2D\",\n",
        "    ),\n",
        "    up_block_types=(\n",
        "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
        "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "lr_scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=lr_warmup_steps,\n",
        "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
        ")\n",
        "\n",
        "\n",
        "def save_eval_images(epoch, model, noise_scheduler):\n",
        "    pipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler)\n",
        "    # Sample some images from random noise (this is the backward diffusion process).\n",
        "    # The default pipeline output type is `List[PIL.Image]`\n",
        "    images = pipeline(\n",
        "        batch_size=16,\n",
        "        generator=torch.Generator(device='cpu').manual_seed(1234), # Use a separate torch generator to avoid rewinding the random state of the main training loop\n",
        "        num_inference_steps =1000\n",
        "    ).images\n",
        "\n",
        "    # Make a grid out of the images\n",
        "    image_grid = make_image_grid(images, rows=4, cols=4)\n",
        "\n",
        "    # Save the images\n",
        "    test_dir = os.path.join(\"/content/\", \"sample_data\")\n",
        "    print (f\"save images to {test_dir}\")\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")\n",
        "    save_model(model,f\"{test_dir}/model_{epoch:04d}\")\n",
        "\n",
        "\n",
        "model=model.to(\"cuda\")\n",
        "\n",
        "progress_bar = tqdm(total=len(train_dataloader)*num_epochs)\n",
        "progress_bar.set_description(f\"Epochs {num_epochs} \")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "        model.train(True)\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            images = batch[\"images\"].to(\"cuda\")\n",
        "            batchsize = images.shape[0]\n",
        "            # create a new noise image with the same size as the training image\n",
        "            noise = torch.randn(images.shape, device=\"cuda\")\n",
        "            # create a random list of timesteps for every image in the batch to train the model on\n",
        "            timesteps = torch.randint(\n",
        "                0, noise_scheduler.config.num_train_timesteps, (batchsize,), device=\"cuda\",\n",
        "                dtype=torch.int64\n",
        "            )\n",
        "            # add noise to all the images\n",
        "            noisy_images = noise_scheduler.add_noise(images, noise, timesteps)\n",
        "            # Predict the noise residual\n",
        "            noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
        "            loss = mse_loss(noise_pred, noise)\n",
        "            loss.backward(loss)\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            progress_bar.update(batchsize)\n",
        "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": step, \"epoch\": epoch}\n",
        "            progress_bar.set_postfix(**logs)\n",
        "        model.eval()\n",
        "        if (epoch+1) % 5 == 0:\n",
        "            save_eval_images(epoch, model, noise_scheduler)\n",
        "\n",
        "progress_bar.close()\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0UBFoAX3sgRr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1VPvQ1b-CebOccqa1O5qGgqwYdYYapuoS",
      "authorship_tag": "ABX9TyPC9Wy8qBV6pVMaQ2VfaQzm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}