{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMWsebqMU7pT/OxEVjrgSo9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ebamberg/research-projects-ml/blob/main/agents_and_routing/examples_react_agent_from_the_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "Install required libraries and start a LLM using OLLAMA in the background"
      ],
      "metadata": {
        "id": "mZF1aggbfNO8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rH-VlVWfG8-",
        "outputId": "232b4080-04e8-4663-c152-c1583bd09ac2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install ollama langchain_community --quiet\n",
        "\n",
        "modelid=\"gemma3:12b\"\n",
        "#emb_modelid=\"mxbai-embed-large\"\n",
        "\n",
        "get_ipython().system_raw(\"curl -fsSL https://ollama.com/install.sh | sh\")\n",
        "get_ipython().system_raw(\"ollama serve &\")\n",
        "get_ipython().system_raw(f\"ollama pull {modelid}\")\n",
        "#get_ipython().system_raw(f\"ollama pull {emb_modelid}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "get_ipython().system_raw(f\"ollama pull {modelid}\")"
      ],
      "metadata": {
        "id": "TMJTDjHLp-Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import rich\n",
        "import logging\n",
        "from rich.logging import RichHandler\n",
        "from rich import print\n",
        "from ollama import chat\n",
        "from ollama import ChatResponse\n",
        "from ollama import Client\n",
        "import re\n",
        "\n",
        "# model=\"llama3.2\"\n",
        "model=\"gemma3:12b\"\n",
        "host=\"http://localhost:11434\"\n",
        "\n",
        "FORMAT = \"%(message)s\"\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO, format=FORMAT, datefmt=\"[%X]\", handlers=[RichHandler()]\n",
        ")\n",
        "\n",
        "log = logging.getLogger(\"simpleagent\")\n",
        "log.setLevel(logging.DEBUG)\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "Think step by step. You run in a loop of THOUGHT, ACTION, PAUSE, OBSERVATION.\n",
        "At the end of the loop you output an ANSWER\n",
        "Use THOUGHT to describe your thoughts about the question you have been asked.\n",
        "Use ACTION to run one of the actions available to you - then return PAUSE.\n",
        "OBSERVATION will be the result of running those actions.\n",
        "\n",
        "Your available actions are:\n",
        "### ACTIONS\n",
        "\n",
        "\n",
        "Example session:\n",
        "\n",
        "Question: What is the mass of Earth times 2?\n",
        "THOUGHT: I need to find the mass of Earth\n",
        "ACTION: get_planet_mass: Earth\n",
        "PAUSE\n",
        "\n",
        "You will be called again with this:\n",
        "\n",
        "OBSERVATION: 5.972e24\n",
        "\n",
        "THOUGHT: I need to multiply this by 2\n",
        "ACTION: calculate: 5.972e24 * 2\n",
        "PAUSE\n",
        "\n",
        "You will be called again with this:\n",
        "\n",
        "OBSERVATION: 1,1944×10e25\n",
        "\n",
        "If you have the answer, output it as the ANSWER.\n",
        "\n",
        "ANSWER: The mass of Earth times 2 is 1,1944×10e25.\n",
        "\n",
        "Now it's your turn:\n",
        "\n",
        "\"\"\".strip()\n",
        "\n",
        "def calculate(operation:str) -> float:\n",
        "    \"\"\"\n",
        "    e.g. calculate: 4 * 7 / 3\n",
        "    Runs a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\n",
        "    \"\"\"\n",
        "    log.info(f\"calculate: {operation}\")\n",
        "    return eval(operation)\n",
        "\n",
        "def get_planet_mass(planet:str)->float:\n",
        "    \"\"\"\n",
        "    e.g. get_planet_mass: Earth\n",
        "    returns weight of the planet in kg\n",
        "    \"\"\"\n",
        "    log.info(f\"return mass of {planet} \")\n",
        "    return 3.285e23\n",
        "\n",
        "class BaseAgent:\n",
        "    \"\"\"\n",
        "    The Base Agent class.\n",
        "    This class holds a memory of previous calls and builds the foundation for agents\n",
        "    \"\"\"\n",
        "    def __init__(self, systemprompt: str=\"\", tools=[]):\n",
        "        self.log=logging.getLogger(\"agent\")\n",
        "        self.systemprompt=systemprompt\n",
        "\n",
        "        toolprompt=\"\"\n",
        "        for tool in tools:\n",
        "            toolprompt=toolprompt+tool.__name__+\":\"+tool.__doc__+\"\\n\\n\"\n",
        "        self.systemprompt=self.systemprompt.replace(\"### ACTIONS\",\"### ACTIONS\\n\\n\"+toolprompt)\n",
        "\n",
        "        log.debug(self.systemprompt)\n",
        "        self.clear_memory()\n",
        "        self.client = Client(\n",
        "         #   host='http://192.168.0.9:11434',\n",
        "            host=host,\n",
        "            headers={'x-some-header': 'some-value'}\n",
        "        )\n",
        "\n",
        "    def __add2Memory(self, role: str, message: str=\"\"):\n",
        "        if message:\n",
        "            self.memory.append({ 'role':role, 'content': message})\n",
        "\n",
        "    def __execute(self) -> str:\n",
        "        self.log.debug(f'calling with history : {self.memory}')\n",
        "        response: ChatResponse = self.client.chat(model=model, messages=self.memory,options={'temperature':0})\n",
        "        self.log.info(response)\n",
        "        self.__add2Memory(response.message.role, response.message.content)\n",
        "        return response.message.content\n",
        "\n",
        "    def __call__(self, prompt:str=\"\"):\n",
        "        if prompt:\n",
        "            self.__add2Memory(\"user\",prompt)\n",
        "        response= self.__execute()\n",
        "        return response\n",
        "\n",
        "    def clear_memory(self):\n",
        "        self.memory=[]\n",
        "        if self.systemprompt:\n",
        "            self.__add2Memory(\"system\", self.systemprompt)\n",
        "\n",
        "class ReActAgent(BaseAgent):\n",
        "    def __init__(self, systemprompt: str=\"\", tools=[]):\n",
        "        super().__init__(systemprompt,tools)\n",
        "        self.max_iterations=10\n",
        "    def __call__(self, prompt:str=\"\"):\n",
        "        loopcount=0\n",
        "        message=prompt\n",
        "        while loopcount<self.max_iterations:\n",
        "            response=super().__call__(message)\n",
        "            if \"PAUSE\" in response and \"ACTION\" in response:\n",
        "                action = re.findall(r\"ACTION: ([a-z_]+): (.+)\", response, re.IGNORECASE)\n",
        "                tool = action[0][0]\n",
        "                arg = action[0][1]\n",
        "                result=eval(f\"{tool}('{arg}')\")\n",
        "                message=f\"OBSERVATION: {result}\"\n",
        "            if \"ANSWER\" in response:\n",
        "                break\n",
        "            loopcount+=1\n",
        "\n",
        "        return response\n",
        "\n",
        "if __name__=='__main__':\n",
        "    log.info (\"----------------\\n\\nSimple agent\\n\\n-----------------\\n\")\n",
        "    log.info(\"launching!\")\n",
        "\n",
        "    agent=ReActAgent(systemprompt=system_prompt,tools=[get_planet_mass,calculate])\n",
        "    response = agent('What is the mass of Mercury times 5?')\n",
        "    print (response)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "cMSzpl-sLfXI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}