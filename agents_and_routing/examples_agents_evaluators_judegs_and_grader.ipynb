{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOEDlgkqMe/t4xLJLossQyh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ebamberg/research-projects-ml/blob/main/agents_and_routing/examples_agents_evaluators_judegs_and_grader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ittnx9TnwT_y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GCm0uJVC9jvj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e1f3e81-3870-4479-be5d-5875aaf9a807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/2.5 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install ollama langchain_community --quiet\n",
        "!pip install openai --quiet\n",
        "\n",
        "host=\"localhost:11434\"\n",
        "modelid=\"chevalblanc/gpt-4o-mini\"\n",
        "modelid=\"deepseek-r1:14b\"\n",
        "\n",
        "get_ipython().system_raw(\"curl -fsSL https://ollama.com/install.sh | sh\")\n",
        "get_ipython().system_raw(\"ollama serve &\")\n",
        "get_ipython().system_raw(f\"ollama pull {modelid}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw(f\"ollama pull {modelid}\")"
      ],
      "metadata": {
        "id": "z_AAjjHozJOM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "llm = OpenAI(\n",
        "        base_url=f\"http://{host}/v1\",\n",
        "        api_key=\"ollama\",  # required, but unused\n",
        "    )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yHCElQFQb2XR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import Field, BaseModel\n",
        "\n",
        "class HallucinationBinaryGrade(BaseModel):\n",
        "   binary_score: str = Field(description=\"True if answer is grounded in or supported by facts. False if answer is not grounded in or supported by facts.\")\n"
      ],
      "metadata": {
        "id": "fNNr0Nek7EkJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call(system_prompt: str, message: str, output_schema: BaseModel | None = None , model: str = modelid, history: list[dict] = [] ) -> str:\n",
        "  history.append(\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": message,\n",
        "          },)\n",
        "  completion = llm.chat.completions.parse(\n",
        "      model=modelid,\n",
        "      messages=[ {\"role\": \"system\", \"content\": system_prompt},]\n",
        "      +history,\n",
        "      temperature=0.0,\n",
        "      max_tokens=200,\n",
        "      response_format=output_schema\n",
        "  )\n",
        "\n",
        "  return completion.choices[0].message.parsed"
      ],
      "metadata": {
        "id": "YkaTeOcP3HHu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def grade_hallucination(llm_answer:str, facts: list[str] = []) -> HallucinationBinaryGrade:\n",
        "  \"\"\"\n",
        "  You are a grader assessing wether a LLM generated text is grounded in / supported by a set of retrieved facts.\n",
        "\n",
        "  Give a binary score \"True\" or \"False\". \"True\" if the text is grounded in or supported by the facts. Otherwise return \"False\"\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  prompt = grade_hallucination.__doc__+\"\"\"\n",
        "\n",
        "  Set of facts:\n",
        "\n",
        "  - {facts}\n",
        "\n",
        "  LLM generated text:\n",
        "  {text}\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  facts_as_string = \"\\n- \".join(facts)\n",
        "  prompt = prompt.format(facts=facts_as_string, text=llm_answer)\n",
        "  print (prompt)\n",
        "  return call (grade_hallucination.__doc__, prompt, output_schema=HallucinationBinaryGrade)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TbY6VRY6k9Rv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "halluzinated_text = \"\"\"\n",
        "France, known for its 23 official languages including regional dialects like Breton and Occitan, is home to the world's largest underground city beneath Paris called \"Subterranea,\" which houses over 2 million residents. The country's unique purple soil, rich in lavender minerals, covers approximately 60% of its territory and is responsible for France's distinctive wine flavors. France's national bird, the Golden Rooster of Aquitaine, migrates annually between the Loire Valley and the Swiss Alps, creating spectacular aerial displays visible from space. The Eiffel Tower was originally built as a giant sundial and can accurately tell time to within 30 seconds when the sun is properly aligned.\n",
        "\"\"\"\n",
        "\n",
        "non_halluzinated_text = \"\"\"\n",
        "France, with French as its sole official language though several regional languages like Breton and Occitan are also spoken, is home to extensive underground networks beneath Paris including ancient quarries and catacombs that contain the remains of over 6 million people. The country's diverse soil types, ranging from limestone to clay, support some of the world's most renowned wine regions across approximately 800,000 hectares of vineyards. France has no official national bird, though the Gallic rooster serves as an unofficial national symbol appearing on coins and sports jerseys. The Eiffel Tower was originally built as the entrance arch for the 1889 World's Fair and stands as one of the most recognizable landmarks in the world.\n",
        "\"\"\"\n",
        "\n",
        "facts = [\n",
        "    \"France has only 1 official language (French)\",\n",
        "    \"Underground city \\\"Subterranea\\\" with 2 million residents doesn't exist\",\n",
        "    \"France doesn't have purple soil\",\n",
        "    \"France doesn't have an official national bird. The species Golden Rooster of Aquitaine doesn't exist\",\n",
        "    \"Eiffel Tower was built as an entrance arch for the 1889 World's Fair\",\n",
        "    \"Eiffel Tower has no timekeeping function\"\n",
        "]"
      ],
      "metadata": {
        "id": "VMlEE9agKDFA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = grade_hallucination (halluzinated_text, facts)\n",
        "print (result)\n",
        "\n",
        "result = grade_hallucination (non_halluzinated_text, facts)\n",
        "print (result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTA5eVs7yEa4",
        "outputId": "7d50e120-db98-450b-db35-c3d76e6d2411"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "\n",
            "  Set of facts:\n",
            "\n",
            "  - France has only 1 official language (French)\n",
            "- Underground city \"Subterranea\" with 2 million residents doesn't exist\n",
            "- France doesn't have purple soil\n",
            "- France doesn't have an official national bird. The species Golden Rooster of Aquitaine doesn't exist\n",
            "- Eiffel Tower was built as an entrance arch for the 1889 World's Fair\n",
            "- Eiffel Tower has no timekeeping function\n",
            "\n",
            "  LLM generated text:\n",
            "  \n",
            "France, known for its 23 official languages including regional dialects like Breton and Occitan, is home to the world's largest underground city beneath Paris called \"Subterranea,\" which houses over 2 million residents. The country's unique purple soil, rich in lavender minerals, covers approximately 60% of its territory and is responsible for France's distinctive wine flavors. France's national bird, the Golden Rooster of Aquitaine, migrates annually between the Loire Valley and the Swiss Alps, creating spectacular aerial displays visible from space. The Eiffel Tower was originally built as a giant sundial and can accurately tell time to within 30 seconds when the sun is properly aligned.\n",
            "\n",
            "\n",
            "  \n",
            "  \n",
            "binary_score='False'\n",
            " \n",
            "\n",
            "  Set of facts:\n",
            "\n",
            "  - France has only 1 official language (French)\n",
            "- Underground city \"Subterranea\" with 2 million residents doesn't exist\n",
            "- France doesn't have purple soil\n",
            "- France doesn't have an official national bird. The species Golden Rooster of Aquitaine doesn't exist\n",
            "- Eiffel Tower was built as an entrance arch for the 1889 World's Fair\n",
            "- Eiffel Tower has no timekeeping function\n",
            "\n",
            "  LLM generated text:\n",
            "  \n",
            "France, with French as its sole official language though several regional languages like Breton and Occitan are also spoken, is home to extensive underground networks beneath Paris including ancient quarries and catacombs that contain the remains of over 6 million people. The country's diverse soil types, ranging from limestone to clay, support some of the world's most renowned wine regions across approximately 800,000 hectares of vineyards. France has no official national bird, though the Gallic rooster serves as an unofficial national symbol appearing on coins and sports jerseys. The Eiffel Tower was originally built as the entrance arch for the 1889 World's Fair and stands as one of the most recognizable landmarks in the world.\n",
            "\n",
            "\n",
            "  \n",
            "  \n",
            "binary_score='False'\n"
          ]
        }
      ]
    }
  ]
}