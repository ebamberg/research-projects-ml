{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMhM59m0vzd7urWJgQCw2Dm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ebamberg/research-projects-ml/blob/main/agents_and_routing/examples_agents_evaluators_judegs_and_grader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ittnx9TnwT_y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GCm0uJVC9jvj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e1f3e81-3870-4479-be5d-5875aaf9a807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/2.5 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install ollama langchain_community --quiet\n",
        "!pip install openai --quiet\n",
        "\n",
        "host=\"localhost:11434\"\n",
        "modelid=\"chevalblanc/gpt-4o-mini\"\n",
        "modelid=\"deepseek-r1:14b\"\n",
        "\n",
        "get_ipython().system_raw(\"curl -fsSL https://ollama.com/install.sh | sh\")\n",
        "get_ipython().system_raw(\"ollama serve &\")\n",
        "get_ipython().system_raw(f\"ollama pull {modelid}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw(f\"ollama pull {modelid}\")"
      ],
      "metadata": {
        "id": "z_AAjjHozJOM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "llm = OpenAI(\n",
        "        base_url=f\"http://{host}/v1\",\n",
        "        api_key=\"ollama\",  # required, but unused\n",
        "    )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yHCElQFQb2XR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import Field, BaseModel\n",
        "\n",
        "class HallucinationBinaryGrade(BaseModel):\n",
        "   binary_score: str = Field(description=\"Text is grounded in / supported by facts. \\\"True\\\" if the text is grounded in or supported by the facts. Otherwise return \\\"False\\\" .\")\n",
        "   explaination: str = Field(description=\"Explain your reasoning.\")\n",
        "   inaccurancies: list[str] = Field(description=\"List any inaccuracies or inconsistencies in the text.\")"
      ],
      "metadata": {
        "id": "fNNr0Nek7EkJ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call(system_prompt: str, message: str, output_schema: BaseModel | None = None , model: str = modelid) -> str:\n",
        "\n",
        "  messages=[\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": message,\n",
        "          },]\n",
        "  completion = llm.chat.completions.parse(\n",
        "      model=modelid,\n",
        "      messages=[ {\"role\": \"system\", \"content\": system_prompt},]\n",
        "      +messages,\n",
        "      temperature=0.0,\n",
        "   #   max_tokens=4096,\n",
        "      response_format=output_schema\n",
        "  )\n",
        "\n",
        "  return completion.choices[0].message.parsed"
      ],
      "metadata": {
        "id": "YkaTeOcP3HHu"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def grade_hallucination(llm_answer:str, facts: list[str] = []) -> HallucinationBinaryGrade:\n",
        "  \"\"\"\n",
        "  You are a grader assessing wether a LLM generated text is grounded in / supported by a set of retrieved facts.\n",
        "\n",
        "  Give a binary score \"True\" or \"False\" and explain your reasoning.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  prompt = grade_hallucination.__doc__+\"\"\"\n",
        "\n",
        "  Set of facts:\n",
        "\n",
        "  - {facts}\n",
        "\n",
        "  LLM generated text:\n",
        "  {text}\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  facts_as_string = \"\\n- \".join(facts)\n",
        "  prompt = prompt.format(facts=facts_as_string, text=llm_answer)\n",
        "  print (prompt)\n",
        "  return call (grade_hallucination.__doc__, prompt, output_schema=HallucinationBinaryGrade)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TbY6VRY6k9Rv"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "halluzinated_text = \"\"\"\n",
        "France, known for its 23 official languages including regional dialects like Breton and Occitan, is home to the world's largest underground city beneath Paris called \"Subterranea,\" which houses over 2 million residents. The country's unique purple soil, rich in lavender minerals, covers approximately 60% of its territory and is responsible for France's distinctive wine flavors. France's national bird, the Golden Rooster of Aquitaine, migrates annually between the Loire Valley and the Swiss Alps, creating spectacular aerial displays visible from space. The Eiffel Tower was originally built as a giant sundial and can accurately tell time to within 30 seconds when the sun is properly aligned.\n",
        "\"\"\"\n",
        "\n",
        "non_halluzinated_text = \"\"\"\n",
        "France, with French as its sole official language though several regional languages like Breton and Occitan are also spoken, is home to extensive underground networks beneath Paris including ancient quarries and catacombs that contain the remains of over 6 million people. The country's diverse soil types, ranging from limestone to clay, support some of the world's most renowned wine regions across approximately 800,000 hectares of vineyards. France has no official national bird, though the Gallic rooster serves as an unofficial national symbol appearing on coins and sports jerseys. The Eiffel Tower was originally built as the entrance arch for the 1889 World's Fair and stands as one of the most recognizable landmarks in the world.\n",
        "\"\"\"\n",
        "\n",
        "facts = [\n",
        "    \"France has only 1 official language (French)\",\n",
        "    \"Underground city \\\"Subterranea\\\" with 2 million residents doesn't exist\",\n",
        "    \"France doesn't have purple soil\",\n",
        "    \"France doesn't have an official national bird. The species Golden Rooster of Aquitaine doesn't exist\",\n",
        "    \"Eiffel Tower was built as an entrance arch for the 1889 World's Fair\",\n",
        "    \"Eiffel Tower has no timekeeping function\"\n",
        "]"
      ],
      "metadata": {
        "id": "VMlEE9agKDFA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = grade_hallucination (halluzinated_text, facts)\n",
        "print (result)\n",
        "\n",
        "result = grade_hallucination (non_halluzinated_text, facts)\n",
        "print (result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTA5eVs7yEa4",
        "outputId": "0b6b5418-ea01-46bf-be4e-dff19278b73c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "  You are a grader assessing wether a LLM generated text is grounded in / supported by a set of retrieved facts.\n",
            "\n",
            "  Give a binary score \"True\" or \"False\" and explain your reasoning. \n",
            "\n",
            "   \n",
            "\n",
            "  Set of facts:\n",
            "\n",
            "  - France has only 1 official language (French)\n",
            "- Underground city \"Subterranea\" with 2 million residents doesn't exist\n",
            "- France doesn't have purple soil\n",
            "- France doesn't have an official national bird. The species Golden Rooster of Aquitaine doesn't exist\n",
            "- Eiffel Tower was built as an entrance arch for the 1889 World's Fair\n",
            "- Eiffel Tower has no timekeeping function\n",
            "\n",
            "  LLM generated text:\n",
            "  \n",
            "France, known for its 23 official languages including regional dialects like Breton and Occitan, is home to the world's largest underground city beneath Paris called \"Subterranea,\" which houses over 2 million residents. The country's unique purple soil, rich in lavender minerals, covers approximately 60% of its territory and is responsible for France's distinctive wine flavors. France's national bird, the Golden Rooster of Aquitaine, migrates annually between the Loire Valley and the Swiss Alps, creating spectacular aerial displays visible from space. The Eiffel Tower was originally built as a giant sundial and can accurately tell time to within 30 seconds when the sun is properly aligned.\n",
            "\n",
            "\n",
            "  \n",
            "  \n",
            "binary_score='False' explaination=\"The LLM's text contains several claims that are not supported by the provided facts. For instance, it mentions France having 23 official languages, which contradicts the fact stating there's only one official language, French. Additionally, the underground city 'Subterranea' with over 2 million residents is explicitly stated as non-existent in the facts. The text also claims that France has purple soil and a national bird called the Golden Rooster of Aquitaine, both of which are false according to the provided information. Furthermore, it incorrectly states that the Eiffel Tower was built as a sundial with timekeeping capabilities, whereas the facts clarify that it serves no timekeeping function.\" inaccurancies=['Claimed France has 23 official languages including Breton and Occitan.', \"Mentioned an underground city 'Subterranea' with over 2 million residents.\", \"Suggested France's soil is purple due to lavender minerals.\", 'Identified the Golden Rooster of Aquitaine as a national bird that migrates between regions.', 'Described the Eiffel Tower as originally built as a sundial with timekeeping accuracy.']\n",
            " \n",
            "  You are a grader assessing wether a LLM generated text is grounded in / supported by a set of retrieved facts.\n",
            "\n",
            "  Give a binary score \"True\" or \"False\" and explain your reasoning. \n",
            "\n",
            "   \n",
            "\n",
            "  Set of facts:\n",
            "\n",
            "  - France has only 1 official language (French)\n",
            "- Underground city \"Subterranea\" with 2 million residents doesn't exist\n",
            "- France doesn't have purple soil\n",
            "- France doesn't have an official national bird. The species Golden Rooster of Aquitaine doesn't exist\n",
            "- Eiffel Tower was built as an entrance arch for the 1889 World's Fair\n",
            "- Eiffel Tower has no timekeeping function\n",
            "\n",
            "  LLM generated text:\n",
            "  \n",
            "France, with French as its sole official language though several regional languages like Breton and Occitan are also spoken, is home to extensive underground networks beneath Paris including ancient quarries and catacombs that contain the remains of over 6 million people. The country's diverse soil types, ranging from limestone to clay, support some of the world's most renowned wine regions across approximately 800,000 hectares of vineyards. France has no official national bird, though the Gallic rooster serves as an unofficial national symbol appearing on coins and sports jerseys. The Eiffel Tower was originally built as the entrance arch for the 1889 World's Fair and stands as one of the most recognizable landmarks in the world.\n",
            "\n",
            "\n",
            "  \n",
            "  \n",
            "binary_score='True' explaination=\"The text accurately reflects the provided facts, incorporating accurate information about France's language, lack of an official national bird, and the purpose behind building the Eiffel Tower. It also correctly mentions that there is no purple soil in France.\" inaccurancies=[\"Mentions 'extensive underground networks beneath Paris including ancient quarries and catacombs' which are not part of the provided facts.\", \"References to 'diverse soil types, ranging from limestone to clay' supporting vineyards is accurate but not part of the given facts.\"]\n"
          ]
        }
      ]
    }
  ]
}